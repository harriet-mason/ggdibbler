---
title: "An introduction to ggdibbler"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ggdibbler-vignette}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(rmarkdown.html_vignette.check_title = FALSE)
```

```{r setup, message=FALSE, warning=FALSE, echo=FALSE}
library(ggdibbler)
library(tidyverse)
library(sf)
library(distributional)
library(patchwork)
library(tidygraph)
library(ggraph)
```


## Why use `ggdibbler`?
It may not be obvious from the outset why we would want this package, after all, there are plenty of geoms and plenty of ways to visualise distributions, so what is the point of this? 

For information to be passed into ggplot2, or any visualisation software, it usually needs to be expressed as data. This restriction prevents us from visualising inputs that are too uncertain to be expressed as single data values. This can include things like estimates, model predictions, bounded values or observations with large measurement error. 

Now, users can simply replace a vector of data with a vector of random variables created using [`distributional`](https://github.com/mitchelloharawild/distributional) and visualise these types of inputs using `ggdibbler`. Any quantifiable uncertainty can be expressed as a distribution, and any distribution can be passed to an existing ggplot2 geom with ggdibbler. Users are not limited by data type (the data can be a random factor, continuous variable, character, etc) or by type of distribution (it can be any theoretical distribution, but also an empirical distribution expressed as a set of samples). 

`ggdibbler` incorporates uncertainty into your graphic as noise and allows you to see how the variation may change the conclusions you take away from your graphics. The power of ggdibbler is in its simplicity and flexibility.  There is no need to learn specific package syntax or new functions and you can pass a distribution to ANY combination of aesthetics in ggplot2. 

The value of `ggdibbler` becomes aparent when we look at a use cases for the software.

## Visualising uncertainty as a function of a plot
If you have taken a look at any `ggdibbler`,  documentation, you may have noticed that it is a replica of the `ggplot2` documentation, right down to the examples. (Almost) every dataset used in a `ggplot2` example has a `ggdibbler` equivalent, every `geom_*` and `stat_*` is replicated with a `geom_*_sample` and `stat_*_sample` function respectively. We repeatedly show uncertain plots alongside their certain counterparts. 
  
The philosophy of `ggdibbler` is that there is no such thing as an uncertainty visualisation (despite us using the phrase repeatedly in the documentation), at least not in isolation. Uncertainty visualisation is the process of incorporating uncertainty into the graphic, such that we can see how that uncertainty changes the take away messages of the plot. Every `ggdibbler` visualisation should have a `ggdplot2` comparison, and vice-versa. This is why the `ggdibbler` examples, function, and data are presented as they are. It is to show what a plot looks like if you replace every single variable with a random variable. Sometimes this makes the dataset a little odd, e.g. why would we be uncertain about literally every single variable in `mtcars`.

```{r}
head(mtcars[,1:3])
head(uncertain_mtcars[,1:3])
```

But the data sets are more... illustrative than practical. Hopefully what the data examples lack in sense, they make up for in exploration, as you can use `ggdibbler` to investigate the (sometimes questionable) ways added uncertainty to the variables. The examples section of this vignette has more practical examples so you can see how the package actually works in practice.

## Using distributional
`ggdibbler` is entirely built upon the R package `distributional` which allows you to store distributions in a `tibble` as distribution objects. I had literally nothing to do with the package, Mitch, Matt, Alex and Rob have my eternal gratitude for writing it, but it met almost all my needs when it came to building `ggdibbler`. If you have a case of uncertainty you want to represent, I would seriously suggest having a poke around in distributional documentation. It is shockingly flexible and I am yet to come across a case of uncertainty *cannot* be represented by `distributional`.

- Bounded values? use `dist_uniform(lowerbound, upperbound)`
- Uncertain predicted classes? use `dist_categorical`
- Want to sample values from a specific vector of values (such as a posterior)? use `dist_sample`

If you have an uncertainty that can be expressed numerically (which is unfortunately the most basic prerequisite for plotting anything, not just uncertainty) then it can almost certainly be captured with a distribution.

All of the data sets in `ggdibbler` already have the distributional component added, but making a distribution vector is shockingly easy. Below is an example where we make normally distributed estimates by wrapping our typical estimate and standard error calculations in a `dist_normal` function.

```{r}
toy_temp_eg <- toy_temp |> 
  group_by(county_name) |>
  summarise(temp_dist = dist_normal(mu = mean(recorded_temp),
                                    sigma = sd(recorded_temp)/sqrt(n())))

print(head(toy_temp_eg))
```

There are also flexibilities that go beyond just the distribution you choose.

- You can transform distributions using `dist_transformed` or make your own distributions if you want (although I am not sure how difficult that is, I have never had to do it).

- You can also mix different distributions together (all the distributions in a variable don't need to be the same family)

- You can have deterministic (there is a deterministic distribution) and distributional objects in the same column.

We will now go onto the `ggdibbler` specific examples (this is not the distributional vignette) but it is definitely worth having a play around in `distributional` so you understand the object before jumping straight into `ggdibbler` plots.


## Examples
### A basic `geom_abline`
The most basic estimate we can have, is the estimate for a simple linear regression. A simple linear regression is actually used in the `ggplot2` documentation as the example for `geom_abline`. 

```{r}
# plot data
p <- ggplot(mtcars, aes(wt, mpg)) + geom_point()
p
# Calculate slope and intercept of line of best fit
# get coef and standard error
summary(lm(mpg ~ wt, data = mtcars))
```

```{r}
# ggplot, just error estimate
p1 <- p + geom_abline(intercept = 37, slope = -5)
# ggdibbler for coef AND standard error
p2 <- p + geom_abline_sample(intercept = dist_normal(37, 1.8), slope = dist_normal(-5, 0.56),
                       times=30, alpha=0.3)
p1 + p2
```

A fun aspect of this approach, is that we can simultaneously see the impact of the variance in the intercept *and* the wt coefficient. 


### A more complicated case with geom_sf
Ok, I am bored, one simple example is. Lets work through a longer and more difficult case of uncertainty visualisation, a spatial visualisation with an uncertain fill. To look at this, we are going to use one of the example data sets that comes with `ggdibbler`,`toy_temp`.

The `toy_temp` data set is a simulated data set that represents observations collected from citizen scientists in several counties in Iowa. Each county has several measurements made by individual scientists at the same time on the same day, but their exact location is not provided to preserve anonymity. Different counties can have different numbers of citizen scientists and the temperature measurements can have a significant amount of variance due to the recordings being made by different people in slightly different locations within the county. Each recorded temperature comes with the county the citizen scientist belongs to, the temperature recording the made, and the scientist's ID number. There are also variables to define spatial elements of the county, such as it's geometry, and the county centroid's longitude and latitude.

```{r}
glimpse(toy_temp)
```


While it is slightly difficult, we can view the individual observations by plotting them to the centroid longitude and latitude (with a little jitter) and drawing the counties in the background for referece.

```{r}
# Plot Raw Data
ggplot(toy_temp) +
  geom_sf(aes(geometry=county_geometry)) +
  ggtitle("ggdibbler some error")+
  geom_jitter(aes(x=county_longitude, y=county_latitude, colour=recorded_temp), 
              width=5000, height =5000, alpha=0.7) +
  scale_colour_distiller(palette = "OrRd") 

```

Typically, we would not visualise the data this way. A much more common approach would be to take the average of each county and display that in a choropleth map, displayed below.


```{r}
# Mean data
toy_temp_mean <- toy_temp |> 
  group_by(county_name) |>
  summarise(temp_mean = mean(recorded_temp)) 
  
# Plot Mean Data
ggplot(toy_temp_mean) +
  ggtitle("ggdibbler some error")+
  scale_fill_distiller(palette = "OrRd") +
  geom_sf(aes(geometry=county_geometry, fill=temp_mean))
```

This plot is fine, but it does loose a key piece of information, specifically the understanding that this mean is an estimate. That means that this estimate has a sampling distribuiton that is invisible to us when we make this visualisation. 


We can see that there is a wave like pattern in the data, but sometimes spatial patterns are a result of significant differences in population, and may disappear if we were to include the variance of the estimates, we can calculate that with the average.

```{r}
# Mean and variance data
toy_temp_est <- toy_temp |> 
  group_by(county_name) |>
  summarise(temp_mean = mean(recorded_temp),
            temp_se = sd(recorded_temp)/sqrt(n())) 
```


Getting an estimate along with its variance is also a common format governments supply data. Just like in our citizen scientist case, this if often done to preserve anonymity. 


The problem with this format of data, is that there is no way for us to include the variance information in the visualisation. We can only visualise the estimate and its variance separately. So, instead of trying to use the estimate and its variance as different values, we combine them as a single distribution variable thanks to the `distributional` package and then can use it with the `ggdibbler` version of `geom_sf`, `geom_sf_sample`.

```{r}
# Distribution
toy_temp_dist <- toy_temp_est |> 
  mutate(temp_dist = dist_normal(temp_mean, temp_se)) |>
  select(county_name, temp_dist) 

# Plot Distribution Data
ggplot(toy_temp_dist) +
  geom_sf_sample(aes(geometry=county_geometry, fill=temp_dist), 
                 times=50, linewidth=0) +
  scale_fill_distiller(palette = "OrRd") 
```

To maintain flexibility, the `geom_sf_sample` does not highlight the original boundary lines, but that can be easily added just by adding another layer.

```{r}
ggplot(toy_temp_dist) + 
  geom_sf_sample(aes(geometry = county_geometry, fill=temp_dist), 
                 linewidth=0, times=50) + 
  scale_fill_distiller(palette = "OrRd") +
  geom_sf(aes(geometry = county_geometry), fill=NA, linewidth=1) 
```

By default, `geom_sf_sample` will subdivide the data. It is unlikely that you will ever notice this default, as the random part in a geom_sf is almost always the fill, but in the unlikely event the random object is the SF part, you will need to change the position to `"identity"` and use `alpha` to manage over plotting. Alternatively, making an animated plot (detailed below) also requires the position to be changed to `"identity"`. Note that unlike `geom_sf_sample`, `geom_polygon_sample` does not have subdivide turned on by default. 

Subdivide is the only position that is specifically created for `ggdibbler`. All the other position systems used are nested positions versions the existing `ggplot2` positions. It was inspired by the pixel map used in [`Vizumap`](https://github.com/lydialucchesi/Vizumap).

###  What about animated plots?
Yeah... you can do that too. Its actually super easy. 

[Hypothetical outcome](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://idl.cs.washington.edu/files/2015-HOPs-PLOS.pdf) plots have been suggested as an uncertainty visualisation technique, but they are also simply a subset of the grammar of nested graphics `ggdibbler` implements. In a HOPs plot, it seems that animations replace our distribution specific position adjustment. There really isn't anything special about the animation element, it is the smae as our other distribution specific position adjustments. Implementing the code is trivially easy when we group our position by drawID (which is exactly what we do in the other position adjustments).

```{r}
library(distributional)
library(ggplot2)
library(gganimate)
# bar chart
hops <- ggplot(uncertain_mpg, aes(class)) +
  geom_bar_sample(aes(fill = drv),
                  position = "stack_identity", times=30) +
  transition_states(after_stat(drawID))
hops
```

### What if my plot uses ggplot2 extensions?
If you represent uncertainty as a distributional object, use the `sample_expand` function, and group by `drawID`, you can apply the signal supression methods to literally any graphic. It won't always work as nicely as `ggdibbler` (please download my package, I need a job) but it works well enough. 

Here is an example with ggraph and uncertain edges represented by a sample of simulated edges. So you have this random data set.

```{r}
set.seed(10)
uncertain_edges <- tibble::tibble(from = sample(5, 20, TRUE),
                    to = sample(5, 20, TRUE),
                    weight = runif(20)) |>
  dplyr::group_by(from, to) |>
  dplyr::mutate(to = distributional::dist_sample(list(sample(seq(from=max(1,to-1), 
                                                 to = min(to+1, 5)), 
                                      replace = TRUE)))) |>
  ungroup()
  
head(uncertain_edges)
```

You need to apply `sample_expand` before you convert the data to a graph, because `ggraph` is all totalitarian about what it will keep in its graph data set, and it doesn't allow distributions. 

```{r}
graph_sample <- uncertain_edges |>
  sample_expand(times=50) |>
  as_tbl_graph()
```

Now, the ideal visualisation would allow us to add transparency with a small amount of jitter to straight lines. That doesn't seem to be possible in ggraph as far as I can tell. It seems that, similar to ggplot2, the lines are actually made up of many individual points, and the line geometry simply interpolates between them. When it applies the jitter, it does it to each point, rather than the line as a whole. So, adding the jitter does work, it just doesn't work exactly as I expected it to (or as I would have liked it to). It does produce an uncertainty visualisation, though.

```{r}
jitter = position_jitter(width=0.01, height=0.01)
ggraph(graph_sample, layout = 'fr', weights = weight) + 
  geom_edge_link(aes(group=drawID), position=jitter, alpha=0.1, 
                 linewidth=0.05) + 
  geom_node_point(size=5)
```

You can mess around with alpha, linewidth, and the times argument in the `sample_expand` function to get something you are happy with. There are some other weird overplotting edge things you can utilise to make the uncertainty implicitly represented by some other aesthetic (like thickness or something), but in general the approaches all look similar (and convey similar information)

```{r}
# uncertainty indicated by transparency
ggraph(graph_sample, layout = 'fr', weights = weight) + 
  geom_edge_link(aes(group=drawID), alpha=0.005,
                 linewidth=2) + 
  geom_node_point()

# Thickness = probability of an edge (thicker = more probable)
ggraph(graph_sample, layout = 'fr', weights = weight) + 
  geom_edge_parallel0(aes(group=drawID),
                      sep = unit(0.05, "mm"), alpha=0.3, linewidth=0.1) + 
  geom_node_point(size=15)
```

## But what about my ggplot2 extension?
Sometimes, when making a `ggdibbler` version of a `ggplot2` base function, I would think to myself "What is the point of this, who on earth needs an uncertain version of `stat_unique`?". I still sometimes think that, but the reason we implemented it with *ALL* ggplot2 functions, is not for the users, but rather for the extenders. If there is a ggplot2 extension that is the child of a base `stat` or base position, a geom_*_sample variation of their function will simply be the child of the ggdibbler variation of the ggplot2 stat, instead of the original ggplot2 code. The only ggplot2 extension that might not work with `ggdibbler` is `ggdist` (for the obvious reason that is is the only other ggplot extension that is designed to take distributional input).

Now, I have made all functions in base `ggplot2` accept uncertain inputs, go me, but the real power of ggplot2 comes from the wealth of extension packages. Now, as fun as I am sure it would be, I am not about to spend the next 5 years making pull requests on every `ggplot2` extension package so that they can all accept random variables. Largely because:

1) Going around making pull requests that forces dependency on *my* package sounds like the early warning signs of a personality disorder
2) (More importantly) that sounds really boring and I don't want to.

Thankfully, the `ggdibbler` approach is so easy if you are the author/maintainer of a `ggplot2` extension and you want it to accept random variables, you can just do it yourself. 

TODO - finish this explanation lol

## Limitations of the Package
There are three primary limitations of the software to keep in mind. First, it only allows you to visualise the distribution as a sample. We are planning to expand this to quantiles in later version, but it is not currently an option. Second, if multiple distributions are passed, they are assumed to be independent. This is also something we are hoping to fix in later versions (possibly with a covariance matrix or a dibble object that represents joint distributions). Finally, replacing a single value with a sample of values (which is functionally what ggdibbler does) results in pervasive overplotting issue. We are hoping to mediate this with nested positions and sensible geom defaults, but this can be difficult to manage depending on the type of plot. In general, we recommend *always* setting `alpha<=0.5` the first time you make a plot.